Model:
  encoding: r50k_base
  model_max_len: 128
  num_stacks: 6
  embed_dim: 1024
  num_heads: 16
  ff_dim: 4096
  dropout: 0.3

Training:
  batch_size: 32
  grad_accumulation_steps: 6
  learning_rate: 4e-4
  warmup_steps: 4000
  epochs: 20
  label_smoothing: 0.1
  num_samples: 3000000
  log_interval: 50
  precision: bf16
  # Token-budget bucketing. Uncomment to enable; batch_size is then ignored for training.
  token_budget: 4096   # ~128 tokens avg * 128 samples, tune to GPU memory
  sort_window: 5000     # ~100x expected samples-per-batch; trades padding vs diversity
